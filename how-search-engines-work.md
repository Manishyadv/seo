# How Search Engines Work?

### Crawling and Indexing

Finding information by crawling

Search engines use software known as "web crawlers" to discover publicly available web pages.

Google's most well-known crawler is called "Googlebot".

Crawlers look at web pages and follow links on those pages, much like you would if you were browsing content on the web. they go from link to ink and bring data about those web pages back to Google servers.

The crawl process begins with a list of web addresses from past crawls and sitemaps provided by website owners.

As the search engines crawlers visit these websites, they look for links for other pages to visit. The software pays special attention to new sites, changes to existing sites and dead links.

Google doesn't accept payment to crawl a site more frequently for their web search results. This ensures they have the best possible results because in the long tun that whats best for users and therefore Google's business.

### Organizing information by indexing

The web is like an ever-growing public library with billions of books and n central filing system



Search engines essentially gather the pages during the craw process and then create an index, so that they know exactly how to look things up.



For example: Much like the index in the back of a book, the Google index included information about words and their locations, When you search, at the most basic level, their algorithms look up your search terms in the index to find the appropriate pages.



The search process gets much more complex from there. When you search for "roses" you don't want a page with the word "roses" on it hundreds of times. You probably want pictures, videos or a list of breeds.



Google's indexing systems note many different aspects of pages, such as when they were published whether they contain pictures and videos, and much more.



With the "Knowledge Graph", they are continuing to go beyond keyword matching to better understand the people, places and things you care about. 





